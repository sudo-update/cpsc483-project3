{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0c1c10",
   "metadata": {},
   "source": [
    "<h6>Fall 2021 - CPSC 483-02<br>\n",
    "Project 3 - Classifiers<br>\n",
    "Sean Javiya <br>\n",
    "Jake Wong <br>\n",
    "Timothy Jan<br>\n",
    "</h6><br>\n",
    "<h3>We are using a Banking Marketing dataset from UC Irvine.</h3><br><ol>\n",
    "<li>The data is related with direct marketing campaigns of a Portuguese banking institution.<br></li> \n",
    "<li>The marketing campaigns were based on phone calls. <br></li>\n",
    "<li>Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.<br></li></ol>\n",
    "\n",
    "Our dataset, bank-additional-full.csv, contains all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014] <br>\n",
    "\n",
    "<h3>Input variables:</h3><br>\n",
    "The archive also contains a text file, <b>\"bank-additional-names.txt\"</b>, which describes the dataset and what each column represents.<br>\n",
    "<b>bank client data:</b><br>\n",
    "1 - age (numeric)<br>\n",
    "2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')<br>\n",
    "3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)<br>\n",
    "4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')<br>\n",
    "5 - default: has credit in default? (categorical: 'no','yes','unknown')<br>\n",
    "6 - housing: has housing loan? (categorical: 'no','yes','unknown')<br>\n",
    "7 - loan: has personal loan? (categorical: 'no','yes','unknown')<br>\n",
    "<b>related with the last contact of the current campaign:</b><br>\n",
    "8 - contact: contact communication type (categorical: 'cellular','telephone')<br>\n",
    "9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')<br>\n",
    "10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')<br>\n",
    "11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.<br>\n",
    "<b>other attributes:</b><br>\n",
    "12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)<br>\n",
    "13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)<br>\n",
    "14 - previous: number of contacts performed before this campaign and for this client (numeric)<br>\n",
    "15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')<br>\n",
    "<b>social and economic context attributes:</b><br>\n",
    "16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)<br>\n",
    "17 - cons.price.idx: consumer price index - monthly indicator (numeric)<br>\n",
    "18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)<br>\n",
    "19 - euribor3m: euribor 3 month rate - daily indicator (numeric)<br>\n",
    "20 - nr.employed: number of employees - quarterly indicator (numeric)<br>\n",
    "\n",
    "<h2>Our aim:</h2><br>\n",
    "We will use the scikit-learn library, which is a higher-level machine learning library \n",
    "that will work with NumPy data, \n",
    "and Pandas, a library that makes it easier to manipulate data. \n",
    "We will explore a variety of classification algorithms, and compare their performance on a “real-world” dataset, which will introduce its own set of challenges.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2904e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2f9a9",
   "metadata": {},
   "source": [
    "<h1>Experiment 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc10c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We examine the first 5 rows of our dataset.\n",
      "Data:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education  default housing loan    contact  \\\n",
       "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1   57   services  married  high.school  unknown      no   no  telephone   \n",
       "2   37   services  married  high.school       no     yes   no  telephone   \n",
       "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4   56   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week  ...  campaign  pdays  previous     poutcome emp.var.rate  \\\n",
       "0   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "1   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "2   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "3   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "4   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0          93.994          -36.4      4.857       5191.0  no  \n",
       "1          93.994          -36.4      4.857       5191.0  no  \n",
       "2          93.994          -36.4      4.857       5191.0  no  \n",
       "3          93.994          -36.4      4.857       5191.0  no  \n",
       "4          93.994          -36.4      4.857       5191.0  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('bank-additional/bank-additional-full.csv', delimiter=';')\n",
    "print(\"We examine the first 5 rows of our dataset.\\nData:\\n\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f425b2",
   "metadata": {},
   "source": [
    "<br>Here we have imported the pandas read_csv module to open our csv file \"<b>bank-additional-full.csv</b>\" and then we load the dataset into a pandas DataFrame.<br><ol>\n",
    "\n",
    "\n",
    "> <i><code>data</code> is a DataFrame and is initialized to hold the values from the csv.</i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc522075",
   "metadata": {},
   "source": [
    "<h1>Experiment 2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f82701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.1, train_size=0.9, random_state=(2021-10-25), shuffle=True, stratify=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b8f91",
   "metadata": {},
   "source": [
    "Here we use <code>sklearn.model_selection.train_test_split()</code> to split the <b>features</b> and <b>target</b> values into separate training and test sets. <br><ol>\n",
    "<li><code>train</code> is our training data.</li>\n",
    "<li><code>test</code> is our test data</li></ol><br>\n",
    "Since the dataset is large, we use <b>90%</b> of the original data as a <b>training set</b>, and <b>10%</b> for <b>testing</b>. <br>\n",
    "\n",
    "> <i>To make sure that our results are reproducible, we pass the <code>keyword argument random_state=(2021-10-25)</code>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ff511",
   "metadata": {},
   "source": [
    "<h1>Experiment 3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeed0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(columns='duration')\n",
    "train = train.drop(columns='duration')\n",
    "\n",
    "y_test = test.pop('y')\n",
    "y_train = train.pop('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19ba28",
   "metadata": {},
   "source": [
    "Per the description in <b>bank-additional-names.txt</b>, the duration “should be discarded if the intention is to have a realistic predictive model.”<br><br>\n",
    "The <b>feature y</b> is the <b>target response</b><br>\n",
    "<ol><li><code>y_train</code> are the y values that correspond to our training data.</li>\n",
    "<li><code>y_test</code> are the y values that correspond to our test data</li></ol><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ad5a9",
   "metadata": {},
   "source": [
    "<h1>Experiment 4</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b28b1fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_test_data = test.iloc[:, :7]\n",
    "categorical_train_data = train.iloc[:, :7]\n",
    "\n",
    "categorical_test_data_knn = categorical_test_data.copy(deep=True)\n",
    "categorical_train_data_knn = categorical_train_data.copy(deep=True)\n",
    "\n",
    "test_df = pd.get_dummies(categorical_test_data, drop_first=True) \n",
    "train_df = pd.get_dummies(categorical_train_data, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad2e6ba",
   "metadata": {},
   "source": [
    "We want to see if we can determine whether a client will subscribe to a term deposit based on what we know about them.<br>\n",
    "Let’s take as </b>features</b> the subset of the input variables described as <b>“bank client data”</b>.<br>\n",
    "<ol>\n",
    "<li><code>categorical_train_data</code> are the categorical features from our training data.</li>\n",
    "<li><code>categorical_test_data</code> are the categorical features from our test data.</li></ol>\n",
    "\n",
    "> <i>Note: we make a copy of these data subsets as <code>categorical_test_data_knn</code> and <code>categorical_train_data_knn</code> for later use in Experiment 8.</i>\n",
    "\n",
    "Most of these features are <b>categorical variables</b> that will need to be encoded before they can be treated as vectors.<br>\n",
    "The simplest way to accomplish this is to use <code>pandas.get_dummies()</code>.<br>\n",
    "\n",
    "> <i>Recall</i> that some algorithms (e.g. logistic regression) will have problems with collinear features, so we set the <code>drop_first keyword argument</code>.\n",
    "\n",
    "<ol>\n",
    "<li><code>test_df</code> is the encoded categorical features from our test data.</li>\n",
    "<li><code>train_df</code> is the encoded categorical features from our training data.</li></ol><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6740c879",
   "metadata": {},
   "source": [
    "<h1>Experiment 5</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18d404d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test percentage correct:  0.8788540907987376\n",
      "train percentage correct:  0.8818689470986538\n"
     ]
    }
   ],
   "source": [
    "nbfit = CategoricalNB()\n",
    "nbfit.fit(train_df, y_train)\n",
    "nb_prediction = nbfit.predict(test_df)\n",
    "nb_train_prediction = nbfit.predict(train_df)\n",
    "\n",
    "testscore = 0\n",
    "for index, value in enumerate(y_test):\n",
    "    if value == nb_prediction[index]:\n",
    "        testscore += 1\n",
    "percentage_score = testscore/len(y_test)\n",
    "print(\"test percentage correct: \", percentage_score)\n",
    "\n",
    "trainscore = 0\n",
    "for index, value in enumerate(y_train):\n",
    "    if value == nb_train_prediction[index]:\n",
    "        trainscore += 1\n",
    "percentage_train_score = trainscore/len(y_train) \n",
    "print(\"train percentage correct: \", percentage_train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201dcac",
   "metadata": {},
   "source": [
    "We use <code>sklearn.naive_bayes.CategoricalNB()</code> to fit a <b>Categorical Naive Bayes classifier</b> to the features we identified in the previous experiment.<br>\n",
    "\n",
    "CategoricalNB implements the categorical naive Bayes algorithm for categorically distributed data. It assumes that each feature has its own categorical distribution.<br>\n",
    "\n",
    "For each feature $i$ in the training set $\\begin{bmatrix}  X \\end{bmatrix}$, CategoricalNB estimates a categorical distribution for each feature $i$ of $\\begin{bmatrix}  X \\end{bmatrix}$ conditioned on the class $y$. The index set of the samples is defined as $J={1,...,m}$, with $m$ as the number of samples.\n",
    "\n",
    "The probability that category $t$ is in feature $i$ given class $c$ is estimated as:<br>\n",
    "\n",
    "> $P(x_{i}=t|y=c;\\alpha)=\\frac{N_{tic}+\\alpha}{N_{c}+\\alpha n_{i}}$<br>\n",
    "\n",
    "where $N_{tic}= |\\{j\\in J | x_{ij} = t, y_{j}=c\\}|$\n",
    " is the number of times category $t$ appears in the samples $x_{i}$\n",
    ", which belong to class $c$, <br>\n",
    "$N_{c} = |\\{j\\in J | y_{j}=c\\}|$ is the number of samples with class c,<br>\n",
    "$\\alpha$ is a smoothing parameter, and <br>\n",
    "$n_{i}$ is the number of available categories of feature $i$.\n",
    "\n",
    "<code>nbfit.fit(train_df, y_train)</code> is our implementation<br>\n",
    "\n",
    "> <i>Recall</i>: <br><code>train_df</code> is the encoded categorical features from our training data.<br><code>y_train</code> is the y (target) values that correspond with the training set.\n",
    "\n",
    "<code>nbfit</code> is our model.\n",
    "\n",
    "Then we score our on both the <b>training</b> and <b>test sets</b>.<br>\n",
    "We compare: the <b>predictions</b> generated by our model when applied to the data setto the actual <b>y (target) value</b> and calculate the percentage that were correctly predicted.<br>\n",
    "<br><h3>This classifier appears relatively accurate.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed524bbf",
   "metadata": {},
   "source": [
    "<h1>Experiment 6</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0577e54",
   "metadata": {},
   "source": [
    " Most of these features are <b>categorical</b>, but <b>\"age\"</b> is a <b>quantitative predictor</b>. <br>\n",
    " Categorical Naive Bayes assumes that each value of the age variable is a separate category.<br>\n",
    " If we actually were to implement our classifier this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "560094f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  78  categories for age in the entire data.  This is unreasonable.\n"
     ]
    }
   ],
   "source": [
    "unique_ages = len(set(data.age))\n",
    "print(\"There are \", unique_ages, \" categories for age in the entire data.  This is unreasonable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54fc722",
   "metadata": {},
   "source": [
    "<h1>Experiment 7</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6918bbfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "age_dict = {1:\"tens\",2:\"twenties\",3:\"thirties\",4:\"forties\",5:\"fifties\",6:\"sixties\",7:\"seventies\",8:\"eighties\",9:\"nineties\"}\n",
    "for ind in categorical_test_data.index:\n",
    "    decade_value = int(categorical_test_data['age'][ind]/10)\n",
    "    categorical_test_data['age'][ind] = age_dict[decade_value]\n",
    "for ind in categorical_train_data.index:\n",
    "    decade_value = int(categorical_train_data['age'][ind]/10)\n",
    "    categorical_train_data['age'][ind] = age_dict[decade_value]\n",
    "    \n",
    "test_df_generations = pd.get_dummies(categorical_test_data, drop_first=True)\n",
    "train_df_generations = pd.get_dummies(categorical_train_data, drop_first=True)\n",
    "\n",
    "test_df_generations['age_nineties'] = 0 #preprocessing to add column of 0's for equal number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d66cf",
   "metadata": {},
   "source": [
    "Here, we split the ages into bins, one per decade.  This serves the purpose of categorically defining age.<br><ol>\n",
    "<li><code>train_df_generations</code> are the categorical features from our training data with the age bins.</li>\n",
    "<li><code>test_df_generations</code> are the categorical features from our test data with the age bins</li></ol><br>\n",
    "\n",
    "> The difference between <code>train_df_generations</code> and <code>categorical_train_data</code> is that the age column has been replaced, from the numerical age value, to the decade-age bin (ie. 56 is replaced with \"fifties\" in the new data <code>train_df_generations</code>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03772356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test percentage correct:  0.6448167030832727\n",
      "train percentage correct:  0.8808168550540884\n"
     ]
    }
   ],
   "source": [
    "nbfit_generations = CategoricalNB()\n",
    "nbfit_generations.fit(train_df_generations, y_train)\n",
    "\n",
    "nb_prediction_generations = nbfit_generations.predict(test_df_generations)\n",
    "nb_train_prediction_generations = nbfit_generations.predict(train_df_generations)\n",
    "\n",
    "testscore = 0\n",
    "for index, value in enumerate(y_test):\n",
    "    if value == nb_prediction_generations[index]:\n",
    "        testscore += 1\n",
    "\n",
    "percentage_score_generations = testscore/len(y_test)\n",
    "print(\"test percentage correct: \", percentage_score_generations)\n",
    "\n",
    "trainscore = 0\n",
    "for index, value in enumerate(y_train):\n",
    "    if value == nb_train_prediction_generations[index]:\n",
    "        trainscore += 1\n",
    "\n",
    "percentage_train_score_generations = trainscore/len(y_train) \n",
    "print(\"train percentage correct: \", percentage_train_score_generations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e49693",
   "metadata": {},
   "source": [
    "Now, we re-train our classifier with these age bins instead of the original age value and measure the percentage score that were correctly predicted (the same scoring system from the previous experiments).<br><br>\n",
    "<code>nbfit_generations.fit(train_df_generations, y_train)</code> is our implementation<br>\n",
    "\n",
    "> <i>Recall</i>: <code>y_train</code> is the y (target) values that correspond with the training set.\n",
    "\n",
    "<code>nbfit_generations</code> is our model.\n",
    "\n",
    "<h3>When applied to the training data, the accuracy is about the same as the model in Experiment 5.<br><br>\n",
    "    However when applied to the test set, the accuracy decreased significantly compared to the model from Experiment 5.</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548e814",
   "metadata": {},
   "source": [
    "<h1>Experiment 8</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2d46611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test percentage correct:  0.8771546491866958\n",
      "train percentage correct:  0.8903935903315439\n"
     ]
    }
   ],
   "source": [
    "test_df_knn = pd.get_dummies(categorical_test_data_knn, drop_first=True) \n",
    "train_df_knn = pd.get_dummies(categorical_train_data_knn, drop_first=True)\n",
    "\n",
    "KNNfit = KNeighborsClassifier()\n",
    "KNNfit.fit(train_df_knn, y_train)\n",
    "kn_prediction = KNNfit.predict(test_df_knn)\n",
    "testscore = 0\n",
    "for index, value in enumerate(y_test):\n",
    "    if value == kn_prediction[index]:\n",
    "        testscore += 1\n",
    "percentage_score_knn = testscore/len(y_test)\n",
    "print(\"test percentage correct: \", percentage_score_knn)\n",
    "\n",
    "kn_train_prediction = KNNfit.predict(train_df_knn)\n",
    "trainscore = 0\n",
    "for index, value in enumerate(y_train):\n",
    "    if value == kn_train_prediction[index]:\n",
    "        trainscore += 1\n",
    "\n",
    "percentage_train_score_knn = trainscore/len(y_train) \n",
    "print(\"train percentage correct: \", percentage_train_score_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2043d0",
   "metadata": {},
   "source": [
    "Here, we start over by encoding the copies of the original categorical data that we made in Experiment 4.<br>\n",
    "<ol>\n",
    "<li><code>test_df_knn</code> are the categorical features from our test data.</li>\n",
    "<li><code>train_df_knn</code> are the categorical features from our training data</li></ol><br>\n",
    "This time, we are going to use <code>sklearn.neighbors.KNeighborsClassifier()</code> to fit a <b>KNN classifier</b> model.<br>\n",
    "\n",
    "This model estimates the conditional probability for class $j$ as the fraction of\n",
    "points in $\\mathcal{N}_{0}$ whose<br> response values equal $j$ :<br>\n",
    "\n",
    "> $Pr(Y=j|X=x_{0})=\\frac{1}{K} \\sum_{i\\in\\mathcal{N}_{0}}I(y_{i}=j)$<br>\n",
    "\n",
    "Given a positive in $K$ and a test observation $x_{0}$, the KNN classifier first identifies the neighbors\n",
    "$K points$ in the training data that are closest to $x_{0}$, represented by  $\\mathcal{N}_{0}$.<br>\n",
    "It then estimates the conditional probability for class $j$ as the fraction of\n",
    "points in $\\mathcal{N}_{0}$ whose response values equal $j$:<br>\n",
    "Finally, KNN applies Bayes rule and classifies the test observation $x_{0}$ to\n",
    "the class with the largest probability. (ie, y = \"yes\" or \"no\")<br>\n",
    "\n",
    "Our implementation is <code>KNNfit.fit(train_df_knn, y_train)</code>\n",
    "\n",
    "> <i>Recall</i> <code>y_train</code> are the y (target) values that correspond with the training data\n",
    "\n",
    "Our model is <code>KNNfit</code>.<br>\n",
    "\n",
    "Finally, we measure the percentage score that were correctly predicted (the same scoring system from the previous experiments).<br>\n",
    "\n",
    "> <i>Note:</i> since KNN requires a pass over the data for each prediction, this classifier may take several moments to score.\n",
    "\n",
    "<h3>This classifier is slightly more accurate than the previous classifiers.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a347f77",
   "metadata": {},
   "source": [
    "<h1>Experiment 9</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7745e4",
   "metadata": {},
   "source": [
    "In the previous experiments, we have found that our test results are suspiciously similar. We take a closer look.<br><ul>\n",
    "<li>How many values in the test set have response \"no\", and how many have response \"yes\"?</li>\n",
    "<li>What would be the score if we simply assumed that no customer ever subscribed to the product?</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27578a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no': 3646, 'yes': 473}\n",
      "test percentage correct if we assume there are no subscribed customers:  0.965768390386016\n",
      "train percentage correct if we assume there are no subscribed customers:  0.9718363052685532\n"
     ]
    }
   ],
   "source": [
    "counts = y_test.value_counts().to_dict()\n",
    "print(counts)\n",
    "testscore = 0\n",
    "for index in range(len(kn_prediction)):\n",
    "    if kn_prediction[index] == 'no':\n",
    "        testscore += 1\n",
    "\n",
    "percentage_score_knn = testscore/len(y_test)\n",
    "print(\"test percentage correct if we assume there are no subscribed customers: \", percentage_score_knn)\n",
    "\n",
    "testscore = 0\n",
    "for index in range(len(kn_train_prediction)):\n",
    "    if kn_train_prediction[index] == 'no':\n",
    "        testscore += 1\n",
    "\n",
    "percentage_score_knn_train = testscore/len(y_train)\n",
    "print(\"train percentage correct if we assume there are no subscribed customers: \", percentage_score_knn_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425524a",
   "metadata": {},
   "source": [
    "<h1>Experiment 10</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "947351f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When assuming no customers are subscribed, we can compare to the actual training data to \n",
      "create the confusion matrix: \n",
      " [[32902     0]\n",
      " [ 4167     0]] \n",
      "and its AUC is:\n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "#zeros like output of ^ \n",
    "zeros_dumb = pd.DataFrame(np.zeros_like(kn_train_prediction))\n",
    "y_train_binary = pd.DataFrame(np.where(y_train.values == 'yes', 1, 0),y_train.index)\n",
    "nos_like = pd.DataFrame(np.where(y_train.values == 'yes', 'no', 'no'),y_train.index)\n",
    "ConfusionMatrix_Dumb = confusion_matrix(y_true= y_train, y_pred= nos_like)\n",
    "RocAuc_score = roc_auc_score(y_train_binary, zeros_dumb)\n",
    "print(\"When assuming no customers are subscribed, we can compare to the actual training data to \\n\"\n",
    "      \"create the confusion matrix: \\n\"\n",
    "      , ConfusionMatrix_Dumb,\n",
    "      \"\\nand its AUC is:\\n\"\n",
    "      , RocAuc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11f4f1",
   "metadata": {},
   "source": [
    "Here we use <code>numpy.zeros_like()</code> to create a target vector representing the output of the “dumb” classifier we obtained in the previous experiment.<br>\n",
    "Then we create a <b>confusion matrix</b> and find its <b>AUC</b>.<br>\n",
    "\n",
    "By definition a <b>confusion matrix</b> $C$ is \n",
    "such that <br>\n",
    "$C_{i,j}$ is equal to the number of observations known to be in group $i$\n",
    "and predicted to be in group $j$.\n",
    "\n",
    "Thus in binary classification:<ul>\n",
    "    <li>the count of true negatives is $C_{0,0}$</li>\n",
    "    <li>the count of false negatives is $C_{1,0}$</li>\n",
    "    <li>the count of true positives is $C_{1,1}$</li>\n",
    "    <li>the count of false positives is $C_{0,1}$</li></ul>\n",
    "Confusion Matrix:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>True Negative</td>\n",
    "        <td>False Negative</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>False Positive</td>\n",
    "        <td>True Positive</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "The overall performance of a classifier, summarized over all possible thresholds, is given by the area under the (ROC)\n",
    "curve (AUC stands for Area Under Curve). An ideal ROC curve will hug the top left corner, so the larger area under the AUC the better the classifier. (p151 in \"An Introduction to Statistical Learning\" 2nd Edition)\n",
    "\n",
    "For a predictor ${f}$, an unbiased estimator of its AUC can be expressed by the following Wilcoxon-Mann-Whitney statistic:\n",
    "$ {\\displaystyle AUC(f)={\\frac {\\sum {t{0}\\in {\\mathcal {D}}^{0}}\\sum {t{1}\\in {\\mathcal {D}}^{1}}{\\textbf {1}}[f(t{0})<f(t{1})]}{|{\\mathcal {D}}^{0}|\\cdot |{\\mathcal {D}}^{1}|}},} $\n",
    "\n",
    "where, ${\\textbf {1}}[f(t{0})<f(t{1})]$ denotes an indicator function which returns 1 iff ${\\displaystyle f(t{0})<f(t{1})}$ otherwise return 0; ${\\mathcal {D}}^{0}$ is the set of negative examples, and ${\\mathcal {D}}^{1}$ is the set of positive examples.<br>\n",
    "\n",
    "\n",
    "First, we use <code>numpy.zeros_like()</code> method to create <code>zeros_dumb</code>, a target vector representing the output of the “dumb” classifier described in the previous experiment(<code>nos_like</code> represents the output of the \"dumb\" classifier).<br>\n",
    "We also create <code>y_train_binary</code>, a copy of y_train where \"yes\" has been replaced with \"1\", and \"no\" has been replaced with \"0\".<br>\n",
    "\n",
    "Then, we use <code>sklearn.metrics.confusion_matric()</code> to find the confusion matrix.<br>\n",
    "Our implementation:<br>\n",
    "<code>confusion_matrix(y_true= y_train, y_pred= nos_like)</code><br>\n",
    "\n",
    "Finally, we use <code>sklearn.metrics.roc_auc_score()</code> to find the AUC.<br>\n",
    "Our implementation:<br>\n",
    "<code>roc_auc_score(y_train_binary, zeros_dumb)</code><br>\n",
    "\n",
    "A score of 0.5 indicates that we are working with an uninformative classifier.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6540d08e",
   "metadata": {},
   "source": [
    "<h1>Experiment 11</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e908616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using the generations model from Experiment 7, we can compare to the actual training data to \n",
      "create the confusion matrix: \n",
      " [[32219   683]\n",
      " [ 3735   432]] \n",
      "and its AUC is:\n",
      " 0.5414565448830108\n",
      "\n",
      "\n",
      "When using the KNN model from Experiment 8, we can compare to the actual training data to \n",
      "create the confusion matrix: \n",
      " [[32432   470]\n",
      " [ 3593   574]] \n",
      "and its AUC is:\n",
      " 0.5617320670877847\n"
     ]
    }
   ],
   "source": [
    "nb_train_predictions_generations_binary = pd.DataFrame(np.where(pd.DataFrame(nb_train_prediction_generations).values == 'yes', 1, 0),pd.DataFrame(nb_train_prediction_generations).index)\n",
    "ConfusionMatrix_generations = confusion_matrix(y_true= y_train, y_pred= nb_train_prediction_generations)\n",
    "RocAuc_score_generations = roc_auc_score(y_train_binary, nb_train_predictions_generations_binary)\n",
    "print(\"When using the generations model from Experiment 7, we can compare to the actual training data to \\n\"\n",
    "      \"create the confusion matrix: \\n\"\n",
    "      , ConfusionMatrix_generations,\n",
    "      \"\\nand its AUC is:\\n\"\n",
    "      , RocAuc_score_generations)\n",
    "\n",
    "kn_train_prediction_binary = pd.DataFrame(np.where(pd.DataFrame(kn_train_prediction).values == 'yes', 1, 0),pd.DataFrame(kn_train_prediction).index)\n",
    "ConfusionMatrix_kn = confusion_matrix(y_true= y_train, y_pred= kn_train_prediction)\n",
    "RocAuc_score_kn = roc_auc_score(y_train_binary, kn_train_prediction_binary)\n",
    "print(\"\\n\\nWhen using the KNN model from Experiment 8, we can compare to the actual training data to \\n\"\n",
    "      \"create the confusion matrix: \\n\"\n",
    "      , ConfusionMatrix_kn,\n",
    "      \"\\nand its AUC is:\\n\"\n",
    "      , RocAuc_score_kn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d321289",
   "metadata": {},
   "source": [
    "Here, we generate the confusion matrices and AUC scores for the models from Experiments 7 and 8, respectively.\n",
    "\n",
    "<h3>These models are not performing that well, in fact, they are barely outperforming the uninformed, dumb classifier.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94431a",
   "metadata": {},
   "source": [
    "<h1>Experiment 12</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c5f2165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of yes and no values after balancing the training set:\n",
      " {'no': 32902, 'yes': 32836}\n"
     ]
    }
   ],
   "source": [
    "train_add_y = pd.concat([train_df, y_train], axis= 1)\n",
    "\n",
    "yes_data = train_add_y.where(cond= train_add_y.y=='yes')\n",
    "yes_data = yes_data.dropna(axis= 0, how= 'all')\n",
    "# yes_data #6.88 x less than data\n",
    "over_sample = yes_data.sample(frac=1, random_state=(2021-10-25))\n",
    "over_sample_part = yes_data.sample(frac=0.88, random_state=(2021-10-25))\n",
    "balanced_train = train_add_y\n",
    "\n",
    "for i in range(6):\n",
    "    balanced_train = balanced_train.append(over_sample)\n",
    "balanced_train = balanced_train.append(over_sample_part)\n",
    "counts = balanced_train.y.value_counts().to_dict()\n",
    "print(\"Number of yes and no values after balancing the training set:\\n\", counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ec8b9",
   "metadata": {},
   "source": [
    "From our results it is clear we are dealing with <b>imbalanced</b> data.<br>\n",
    "One of the easiest ways to deal with an unbalanced dataset is <b>random oversampling</b>.<br>\n",
    "We use <code>pandas.DataFrame.where()</code> and <code>pandas.DataFrame.sample()</code> to generate balanced training sets by weighting the values with response y = \"yes\" more heavily (random oversampling). <br>\n",
    "We weight the \"yes\" values at <b>6.88</b> times in order to get an as close to even amount of y = \"yes\" and \"no\" values.\n",
    "\n",
    " > <i>To make sure that our results are reproducible, we pass the <code>keyword argument random_state=(2021-10-25)</code>.\n",
    "    \n",
    "Finally, we verify that the y = \"yes\" and y = \"no\" values are relatively <b>balanced</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98127d",
   "metadata": {},
   "source": [
    "<h1>Experiment 13</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b175250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When balancing the data and then re-training our generation model from Experiment 7:\n",
      "\n",
      "test percentage correct:  0.3046856033017723\n",
      "train percentage correct:  0.6057835650613039\n"
     ]
    }
   ],
   "source": [
    "y_balanced_train = balanced_train.pop('y')\n",
    "balanced_train_gen = balanced_train.copy(deep=True)\n",
    "\n",
    "for ind in balanced_train_gen.index:\n",
    "    if isinstance(balanced_train_gen['age'][ind], pd.Series):\n",
    "        decade_value = (balanced_train_gen['age'][ind].iloc[0])\n",
    "        if decade_value in age_dict.values():\n",
    "            decade_value = decade_value\n",
    "        else:\n",
    "            decade_value = int(int(decade_value) / 10)\n",
    "            decade_value = age_dict[decade_value]\n",
    "    else:\n",
    "        decade_value = int(balanced_train_gen['age'][ind] / 10)\n",
    "        decade_value = age_dict[decade_value]\n",
    "    balanced_train_gen['age'][ind] = decade_value\n",
    "\n",
    "test_gen = pd.get_dummies(test, drop_first=True)\n",
    "balanced_train_gen = pd.get_dummies(balanced_train_gen, drop_first=True)\n",
    "\n",
    "test_gen['age_nineties'] = 0 #preprocessing to add column of 0's for equal number of columns\n",
    "\n",
    "\n",
    "nbfit_balanced_gen = CategoricalNB()\n",
    "nbfit_balanced_gen.fit(balanced_train_gen, y_balanced_train)\n",
    "nb_predict_balanced_gen = nbfit_balanced_gen.predict(balanced_train_gen)\n",
    "nb_predict_balanced_gen_test = nbfit_balanced_gen.predict(test_df_generations)\n",
    "print(\"When balancing the data and then re-training our generation model from Experiment 7:\\n\")\n",
    "testscore = 0\n",
    "for index, value in enumerate(y_test):\n",
    "    if value == nb_predict_balanced_gen_test[index]:\n",
    "        testscore += 1\n",
    "percentage_score_balanced_generations = testscore/len(y_test)\n",
    "print(\"test percentage correct: \", percentage_score_balanced_generations)\n",
    "\n",
    "trainscore = 0\n",
    "for index, value in enumerate(y_balanced_train):\n",
    "    if value == nb_predict_balanced_gen[index]:\n",
    "        trainscore += 1\n",
    "percentage_train_score_balanced_generations = trainscore/len(y_balanced_train) \n",
    "print(\"train percentage correct: \", percentage_train_score_balanced_generations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1aa3a",
   "metadata": {},
   "source": [
    "First, we encode our balanced dataset in the same manner as <b>Experiment 7</b>(with the age-decade bins). \n",
    "<br><br>\n",
    "<code>balanced_train_gen</code> are the encoded categorical features from our training data, but the age column has been replaced from the numerical age value, to the decade-age bin (ie. 56 is replaced with \"fifties\" in the new data, like Experiment 7).<br><br>\n",
    "    \n",
    "<i>The features are encoded with <code>pandas.get_dummies()</code></i><br>\n",
    "\n",
    "Now, we re-train our classifier with these age bins instead of the original age value, using the balanced data set.<br>\n",
    "Our model is <code>nbfit_balanced_gen</code>.<br>\n",
    "Our implementation is <code>nbfit_balanced_gen.fit(balanced_train_gen, y_balanced_train)</code><br>\n",
    "<code>y_balanced_train</code> is the <b>y (target) values</b> that correspond with the new, <b>balanced training set</b>.\n",
    "\n",
    "> <i>Recall</i>: <br><code>test_df_generations</code> is the encoded categorical features from our test data from Experiment 4.<br>\n",
    "\n",
    "Finally, we measure the percentage score that were correctly predicted (the same scoring system from the previous experiments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33580ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When balancing the data and then re-training our generation model from Experiment 8:\n",
      "\n",
      "test percentage correct:  0.6385044913814033\n",
      "train percentage correct:  0.7686117618424655\n"
     ]
    }
   ],
   "source": [
    "balanced_KNNfit = KNeighborsClassifier()\n",
    "balanced_KNNfit.fit(balanced_train, y_balanced_train)\n",
    "balanced_kn_prediction = balanced_KNNfit.predict(test_df_knn)\n",
    "balanced_kn_train_prediction = balanced_KNNfit.predict(balanced_train)\n",
    "\n",
    "print(\"When balancing the data and then re-training our generation model from Experiment 8:\\n\")\n",
    "testscore = 0\n",
    "for index, value in enumerate(y_test):\n",
    "    if value == balanced_kn_prediction[index]:\n",
    "        testscore += 1\n",
    "percentage_score_knn_balanced = testscore/len(y_test)\n",
    "print(\"test percentage correct: \", percentage_score_knn_balanced)\n",
    "\n",
    "trainscore = 0\n",
    "for index, value in enumerate(y_balanced_train):\n",
    "    if value == balanced_kn_train_prediction[index]:\n",
    "        trainscore += 1\n",
    "\n",
    "percentage_train_score_knn_balanced = trainscore/len(y_balanced_train) \n",
    "print(\"train percentage correct: \", percentage_train_score_knn_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a4dbb",
   "metadata": {},
   "source": [
    "First, we re-train our KNN classifier using the balanced data set.<br>\n",
    "Our implementation is <code>balanced_KNNfit.fit(balanced_train, y_balanced_train)</code><br>\n",
    "> <i>Recall:</i> <code>y_balanced_train</code> is the <b>y (target) values</b> that correspond with the <b>balanced training set</b>.<br>  &emsp;&emsp;&emsp;   <code>balanced_train</code> is the balanced training set.<br>\n",
    "\n",
    "Our model is <code>balanced_KNNfit</code>.<br>\n",
    "Finally, we measure the percentage score that were correctly predicted (the same scoring system from the previous experiments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "582ad1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using the new balanced generations model , we create the confusion matrix: \n",
      " [[20089 12813]\n",
      " [13102 19734]] \n",
      "and its AUC is:\n",
      " 0.6057787539305328\n",
      "\n",
      "\n",
      "When using the new balanced KNN model, we create the confusion matrix: \n",
      " [[22942  9960]\n",
      " [ 5251 27585]] \n",
      "and its AUC is:\n",
      " 0.7686834470179584\n"
     ]
    }
   ],
   "source": [
    "nb_balanced_train_pred_gen_binary = pd.DataFrame(np.where(pd.DataFrame(nb_predict_balanced_gen).values == 'yes', 1, 0),pd.DataFrame(nb_predict_balanced_gen).index)\n",
    "y_balanced_train_binary = pd.DataFrame(np.where(y_balanced_train.values == 'yes', 1, 0),y_balanced_train.index)\n",
    "ConfusionMatrix_balanced_generations = confusion_matrix(y_true= y_balanced_train, y_pred= nb_predict_balanced_gen)\n",
    "RocAuc_score_balanced_generations = roc_auc_score(y_balanced_train_binary, nb_balanced_train_pred_gen_binary)\n",
    "print(\"When using the new balanced generations model , we create the confusion matrix: \\n\"\n",
    "      , ConfusionMatrix_balanced_generations,\n",
    "      \"\\nand its AUC is:\\n\"\n",
    "      , RocAuc_score_balanced_generations)\n",
    "\n",
    "kn_balanced_train_pred_binary = pd.DataFrame(np.where(pd.DataFrame(balanced_kn_train_prediction).values == 'yes', 1, 0),pd.DataFrame(balanced_kn_train_prediction).index)\n",
    "ConfusionMatrix_kn_balanced = confusion_matrix(y_true= y_balanced_train, y_pred= balanced_kn_train_prediction)\n",
    "RocAuc_score_kn_balanced = roc_auc_score(y_balanced_train_binary, kn_balanced_train_pred_binary)\n",
    "print(\"\\n\\nWhen using the new balanced KNN model, we create the confusion matrix: \\n\"\n",
    "      , ConfusionMatrix_kn_balanced,\n",
    "      \"\\nand its AUC is:\\n\"\n",
    "      , RocAuc_score_kn_balanced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc26060f",
   "metadata": {},
   "source": [
    "Here, we generate the confusion matrices and AUC scores for the balanced generations and KNN models, respectively.<br>\n",
    "<h3>The KNN classifier generated when training on the balanced data set performs better.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45585d25",
   "metadata": {},
   "source": [
    "<h1>Experiment 14</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e35bb9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using the socio-economic data and then training a Gausian Naive Bayes model:\n",
      "\n",
      "test percentage correct:  0.7193493566399611\n",
      "train percentage correct:  0.7199816558310178\n",
      "\n",
      "\n",
      "When using this model, we create the confusion matrix: \n",
      " [[23717  9185]\n",
      " [ 1195  2972]] \n",
      "and its AUC is:\n",
      " 0.7170302906069623\n"
     ]
    }
   ],
   "source": [
    "economical_test_data = test.iloc[:, 14:]\n",
    "economical_train_data = train.iloc[:, 14:]\n",
    "\n",
    "nbfit_eco = GaussianNB()\n",
    "nbfit_eco.fit(economical_train_data, y_train)\n",
    "nb_prediction_eco = nbfit_eco.predict(economical_test_data)\n",
    "nb_prediction_eco_train = nbfit_eco.predict(economical_train_data)\n",
    "print(\"When using the socio-economic data and then training a Gausian Naive Bayes model:\\n\")\n",
    "\n",
    "testscore = 0\n",
    "for index, value in enumerate(y_test):\n",
    "    if value == nb_prediction_eco[index]:\n",
    "        testscore += 1\n",
    "percentage_score_eco = testscore/len(y_test)\n",
    "print(\"test percentage correct: \", percentage_score_eco)\n",
    "\n",
    "\n",
    "trainscore = 0\n",
    "for index, value in enumerate(y_train):\n",
    "    if value == nb_prediction_eco_train[index]:\n",
    "        trainscore += 1\n",
    "\n",
    "percentage_score_eco_train = trainscore/len(y_train) \n",
    "print(\"train percentage correct: \", percentage_score_eco_train)\n",
    "\n",
    "nb_prediction_eco_train_binary = pd.DataFrame(np.where(pd.DataFrame(nb_prediction_eco_train).values == 'yes', 1, 0),pd.DataFrame(nb_prediction_eco_train).index)\n",
    "ConfusionMatrix_nb_eco = confusion_matrix(y_true= y_train, y_pred= nb_prediction_eco_train)\n",
    "RocAuc_score_eco = roc_auc_score(y_train_binary, nb_prediction_eco_train_binary)\n",
    "print(\"\\n\\nWhen using this model, we create the confusion matrix: \\n\"\n",
    "      , ConfusionMatrix_nb_eco,\n",
    "      \"\\nand its AUC is:\\n\"\n",
    "      , RocAuc_score_eco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5b5f5",
   "metadata": {},
   "source": [
    "Now, we take as features the subset of the input variables described as “social and economic context attributes”.<br>\n",
    "<ol>\n",
    "<li><code>economical_train_data</code> are the socio-economic features from our training data.</li>\n",
    "<li><code>economical_test_data</code> are the socio-economic features from our test data</li></ol><br>\n",
    "\n",
    "We use <code>sklearn.naive_bayes.GaussianNB()</code> to fit a <b>Gaussian Naive Bayes classifier</b> to the features that are described as socio-economic features, because these features are quantitative.<br>\n",
    "\n",
    "<code>GaussianNB()</code> implements the Gaussian Naive Bayes algorithm for classification of quantitatively distributed data.<br>\n",
    "The likelihood of the features is assumed to be Gaussian.<br>\n",
    "The parameters ${σ_y}$ and ${μ_y}$ are estimated using maximum likelihood.\n",
    "\n",
    "$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$\n",
    "\n",
    "<br>\n",
    "<code>nbfit_eco.fit(economical_train_data, y_train)</code> is our implementation<br>\n",
    "\n",
    "> <i>Recall</i>:<code>y_train</code> is the y (target) values that correspond with the training set.\n",
    "\n",
    "<code>nbfit_eco</code> is our model.\n",
    "\n",
    "Then we measure the percentage score that were correctly predicted (the same scoring system from the previous experiments) and generate the <b>confusion matrix</b> and <b>AUC score</b>.<br>\n",
    "<h3>This model predicts y (target) values relatively accurately.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f42fb",
   "metadata": {},
   "source": [
    "<h1>Experiment 15</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ae341ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of yes and no values after balancing the training set:\n",
      " {'no': 32902, 'yes': 32836}\n",
      "When balancing using the socio-economic data, balancing it, and then training a Gausian Naive Bayes model:\n",
      "\n",
      "test percentage correct:  0.7193493566399611\n",
      "train percentage correct:  0.7171194742766741\n",
      "\n",
      "\n",
      "When using this model, we create the confusion matrix: \n",
      " [[23717  9185]\n",
      " [ 9411 23425]] \n",
      "and its AUC is:\n",
      " 0.7171157375414597\n"
     ]
    }
   ],
   "source": [
    "eco_train_add_y = pd.concat([economical_train_data, y_train], axis= 1)\n",
    "\n",
    "eco_yes_data = eco_train_add_y.where(cond= eco_train_add_y.y=='yes')\n",
    "eco_yes_data = eco_yes_data.dropna(axis= 0, how= 'all')\n",
    "# yes_data #6.88 x less than data\n",
    "eco_over_sample = eco_yes_data.sample(frac=1, random_state=(2021-10-25))\n",
    "eco_over_sample_part = eco_yes_data.sample(frac=0.88, random_state=(2021-10-25))\n",
    "eco_balanced_train = eco_train_add_y\n",
    "\n",
    "for i in range(6):\n",
    "    eco_balanced_train = eco_balanced_train.append(eco_over_sample)\n",
    "eco_balanced_train = eco_balanced_train.append(eco_over_sample_part)\n",
    "counts = eco_balanced_train.y.value_counts().to_dict()\n",
    "print(\"Number of yes and no values after balancing the training set:\\n\", counts)\n",
    "\n",
    "y_eco_balanced_train = eco_balanced_train.pop('y')\n",
    "\n",
    "nbfit_eco_balanced = GaussianNB()\n",
    "nbfit_eco_balanced.fit(eco_balanced_train, y_eco_balanced_train)\n",
    "nb_prediction_eco_balanced = nbfit_eco_balanced.predict(economical_test_data)\n",
    "nb_prediction_eco_train_balanced = nbfit_eco_balanced.predict(eco_balanced_train)\n",
    "print(\"When balancing using the socio-economic data, balancing it, and then training a Gausian Naive Bayes model:\\n\")\n",
    "\n",
    "testscore = 0\n",
    "for index, value in enumerate(y_test):\n",
    "    if value == nb_prediction_eco_balanced[index]:\n",
    "        testscore += 1\n",
    "percentage_score_eco_balanced = testscore/len(y_test)\n",
    "print(\"test percentage correct: \", percentage_score_eco_balanced)\n",
    "\n",
    "trainscore = 0\n",
    "for index, value in enumerate(y_eco_balanced_train):\n",
    "    if value == nb_prediction_eco_train_balanced[index]:\n",
    "        trainscore += 1\n",
    "percentage_score_eco_train_balanced = trainscore/len(y_eco_balanced_train) \n",
    "print(\"train percentage correct: \", percentage_score_eco_train_balanced)\n",
    "\n",
    "nb_prediction_eco_train_balanced_binary = pd.DataFrame(np.where(pd.DataFrame(nb_prediction_eco_train_balanced).values == 'yes', 1, 0),pd.DataFrame(nb_prediction_eco_train_balanced).index)\n",
    "ConfusionMatrix_nb_eco_balanced = confusion_matrix(y_true= y_eco_balanced_train, y_pred= nb_prediction_eco_train_balanced)\n",
    "RocAuc_score_eco_balanced = roc_auc_score(y_balanced_train_binary, nb_prediction_eco_train_balanced_binary)\n",
    "print(\"\\n\\nWhen using this model, we create the confusion matrix: \\n\"\n",
    "      , ConfusionMatrix_nb_eco_balanced,\n",
    "      \"\\nand its AUC is:\\n\"\n",
    "      , RocAuc_score_eco_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5581072",
   "metadata": {},
   "source": [
    "Now, we fit a Gaussian Naive Bayes model after balancing the training data with <b>random oversampling</b>.<br>\n",
    "\n",
    "<ol>\n",
    "<li><code>eco_balanced_train</code> is the balanced socio-economic features from our training data after random oversampling.</li>\n",
    "<li><code>y_eco_balanced_train</code> is the corresponding y values.</li></ol>\n",
    "\n",
    "<code>nbfit_eco_balanced.fit(eco_balanced_train, y_eco_balanced_train)</code> is our implementation<br>\n",
    "\n",
    "<code>nbfit_eco_balanced</code> is our model.\n",
    "\n",
    "Finally, we measure the percentage score that were correctly predicted (the same scoring system from the previous experiments) and generate the <b>confusion matrix</b> and <b>AUC score</b>.<br>\n",
    "\n",
    "<h3>The results change very slightly if the training set is balanced.</h3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
